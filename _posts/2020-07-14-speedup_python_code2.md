---
title: Evaluation pipeline for a production ready RAG
layout: post
use_toc: true
excerpt: How to build a dataset to evaluate a RAG?
---
## System Design

<div style="display: flex; justify-content: center; padding-top: 20px; padding-bottom: 20px;">
    <img src="{{ site.baseurl }}/images/LLMOps/system_design.png">
</div>

Evaluating RAG models involves a systematic process that can be broken down into several steps. Below is a detailed guide based on the key components you've outlined.

### 1. Generate a Dataset of Questions
#### 1.1 Data Processing
- **Data**: Select a dataset that is relevant to the domain in which the RAG model will be applied.
- **Split**: Divide the dataset into manageable chunks to improve the efficiency of the question-generation process.
- **Chunks**: Individually process each chunk to generate questions.

#### 1.2 Question Generation
- **Generate 5 questions**: Aim to generate a set of five questions from each data chunk.
- **Questions for each chunk**: Collate the questions generated from each data chunk.
- **Create Encoded Embeddings**: Convert the generated questions into vector space embeddings for similarity searches.
- **Run Similarity Search**: Use the encoded embeddings to identify similar questions or to perform deduplication.
- **Questions-to-Questions with Score**: Evaluate the similarity search results by scoring the quality and relevance of the questions.

#### 1.3 Question Filtering and Finalization
- **Filter with Threshold**: Implement a threshold to filter out questions that do not meet the required similarity score, to ensure question uniqueness and relevance.
- **Cleaned Questions**: Compile the questions that exceed the threshold into a cleaned dataset.
- **Combine chunks at random and generate questions**: Integrate questions from various chunks randomly to enhance diversity and create a final set of questions.
- **Add questions generated by multiple chunks**: Incorporate questions that are common across multiple chunks for robustness.
- **Final Set of Questions (20-100)**: The final output is a refined set of 20 to 100 questions that are ready for the evaluation phase.

### 2. RAG

- **Question as a Vector**: Transform the question into a vector format.
- **Find the Most Relevant Chunks**: Search the vector store, which is a database of encoded vectors, to retrieve the most pertinent information chunks related to the question.
- **Context**: Supply the RAG model with context drawn from the retrieved chunks.
- **Top N Results**: Have the model retrieve the top N most relevant answers or results.
- **GPT Prompt**: Pass the generated prompt to the GPT component of the RAG model.
- **Answer**: Utilize GPT to generate an answer based on the given prompt and the context provided.

### 3. Evaluate the Outcome

- **Question**: Reference the original question for comparison.
- **Answer**: Examine the answer generated by the RAG model.
- **Context**: Verify the accuracy and relevancy of the context used to produce the answer.
- **Prompt**: Confirm that the prompt was appropriately formed and is pertinent to the question.
- **RAGAs, BertScore, etc.**: Employ evaluation metrics such as RAGAs and BertScore to assess the quality of the answers.
- **Scores**: Aggregate the scores from the different metrics to gauge the RAG model's performance.

The aforementioned steps are essential to ensure that the RAG model is thoroughly and accurately tested.

## Dashboard
<div style="display: flex; justify-content: center; padding-top: 20px; padding-bottom: 20px;">
    <img src="{{ site.baseurl }}/images/LLMOps/dashboard.png">
</div>
Dashboard could be implemented using superset or metabase.

## Parameters
### Chunking
  - Chunk size
  - Chunk overlap
  - Embedding model
### Retrival
  - Number of results
  - Similarity threshold
  - Retrival Strategy (BM25, cosine similarity, hybrid etc.)
  - Reranking
### Generation
  - LLM Model
  - Prompt
  - Temperature

## Metrics
### Chunking
this is difficult to measure, therefore we can only evaluate end-to-end performance. 

### Retrival
focuses on how accurately we can retrieve necessary chunks from vector store.
- #### Mean Average Precision
- evaluates System 1’s result for the query to be 0.6 and System 2’s to be 0.1. 
![image](./images/LLMOps/retrival.png)
- #### Ragas
  - Context Precision
    - evaluates whether all of the ground-truth relevant items present in the `context` are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the `question` and the `contexts`.

### Generation
- #### BLEU/ROUGE
- #### BERTScore
- #### Ragas
  - Answer Relevance 
    - focuses on assessing how pertinent the generated answer is to the given prompt. This metric is computed using the `question` and the `answer`.     
  - Faithfulness
    - This measures the factual consistency of the generated answer against the given context. It is calculated from `answer` and retrieved `context`.

## Problems:
### Problem: Evaluation Speed
- The current evaluation process is slow, taking approximately one minute per question for all metrics.
### Solutions
- **Parallel Processing**: Run evaluations in parallel for each question to generate tokens more efficiently.
- **Threading**: Utilize threads in newer models to enhance processing speed.

### Problem: Cost-Effectiveness and Efficiency
- Due to the time-intensive nature of the analysis and associated costs, it is crucial to identify and address issues early in the process.
### Solutions
- **Fast Fail Approach**: Implement mechanisms to quickly determine if there are problems with the test set of questions.
    - This could include identifying if the test set:
        - Contains an excessive number of questions.
- **Two-Tiered Evaluation Process**:
    - **Fast Test Set**: Begin with a quick evaluation using a top 20 question set that every customer should be able to handle.
        - If the set meets a threshold, such as 90% accuracy on specified metrics, proceed to a more comprehensive analysis.
    - **Extended Evaluation**:
        - Reserve longer, more detailed tests for periods following major releases rather than after every commit or pull request. This approach ensures that the end product is polished for customer use.

### Problem: User Interaction Analysis
### Solutions
- **Analytics Integration**: Implement analytics tools, like Langfuse, to monitor and understand how users interact with your RAG.

## Problem: Focus and Relevance of Questions
### Solutions
- **Question Clustering**: Organize questions into cluster groups and prioritize evaluation on the top 5 clusters.
- **Topic Modeling**: Employ techniques like Berttopic to effectively group questions into relevant topics for targeted evaluation.

### Literature 
- [Hands-On Large Language Models(Jay Alammar, Maarten Grootendorst)](https://learning.oreilly.com/api/v1/continue/9781098150952/)
