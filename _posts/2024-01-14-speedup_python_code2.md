---
title: Evaluation pipeline for a production ready RAG
layout: post
use_toc: true
excerpt: How to build a dataset to evaluate a RAG?
---
### System Design

<div style="display: flex; justify-content: center; padding-top: 20px; padding-bottom: 20px;">
    <img src="{{ site.baseurl }}/images/LLMOps/system_design.png">
</div>

Evaluating RAG models involves a systematic process that can be broken down into several steps. Below is a detailed guide based on the key components you've outlined.

#### Question Generation
- **Generate N questions**: with a simple prompt generate a set of N questions from each data chunk.
- **Embed Questions**: remove duplicated questions and convert the generated questions into vectors for similarity searches.
- **Questions-to-Questions with Score**: run the similarity search results between questions.

#### Question Filtering and Finalization
- **Filter with Threshold**: set a threshold to filter out questions are very similar to each other to ensure question uniqueness and relevance.
- **Combine chunks at random and generate questions**: Integrate questions from various chunks randomly to enhance diversity and create a final set of questions.

While developing questions from individual chunks of text, we encountered an issue where each question was linked to a single, isolated chunk. 
We would to have questions that are comming form chunks that are not sequential. To enhance our question set, we're now focusing on formulating questions that draw upon multiple chunks, even if these chunks are located far apart. 
From **Questions-to-Questions with Score** step we know which chunks generates similar questions, so we could assume that these chunks could be connected to each other and be helpful while answering question. 
Finnaly, we could create questions that are based on multiple relevant chunks, combining relevant chunks and then use a language model to generate questions based on these chunks.

- **Add questions generated by multiple chunks**: Incorporate questions that are common across multiple chunks for robustness.
- **Final Set of Questions (20-50)**: I would suggest to have a small set of questions in the beginning, to be able to evaluate yourself. Then you could increase the number of questions.

#### RAG

- **Find the Most Relevant Chunks**: Search in the vector store the most relevant chunks related to the question.
- **Answer**: Utilize GPT to generate an answer based on the question, given prompt and the context provided.

#### Evaluate the Outcome

- **Score = Metric(Question, Answer, Context, Prompt)**
You can find a list of metrics below.

<div style="display: flex; justify-content: center; padding-top: 40px; padding-bottom: 40px;">
    <img src="{{ site.baseurl }}/images/LLMOps/dashboard.png" style="width: 75%;"/>
</div>

### Parameters
- **Chunking**
  - Chunk size
  - Chunk overlap
  - Embedding model
- **Retrival**
  - Number of results
  - Similarity threshold
  - Retrival Strategy (BM25, cosine similarity, hybrid etc.)
  - Reranking
- **Generation**
  - LLM Model
  - Prompt
  - Temperature

### Metrics
**Chunking**   
this is difficult to measure, therefore we can only evaluate end-to-end performance. 

**Retrival**   
focuses on how accurately we can retrieve necessary chunks from vector store.
- Ragas
  - Context Precision
    - evaluates whether all of the ground-truth relevant items present in the `context` are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the `question` and the `contexts`.

- Mean Average Precision

<div style="display: flex; justify-content: center; padding-top: 20px; padding-bottom: 20px;">
    <img src="{{ site.baseurl }}/images/LLMOps/retrival2.png" style="width: 50%;"/>
    <img src="{{ site.baseurl }}/images/LLMOps/retrival.png" style="width: 50%;"/>
</div>

**Generation**
- BLEU/ROUGE
- BERTScore
- Ragas
  - Answer Relevance 
    - focuses on assessing how pertinent the generated answer is to the given prompt. This metric is computed using the `question` and the `answer`.     
  - Faithfulness
    - This measures the factual consistency of the generated answer against the given context. It is calculated from `answer` and retrieved `context`.

### Problems
- **Problem: Evaluation Speed**   
The current evaluation process is slow, taking approximately one minute per question for all metrics.   
**Solutions**:
  - Parallel Processing: Run evaluations in parallel for each question, you can have multiple tokens for openai.
  - Threading: Utilize threads in newer models to enhance processing speed.

- **Problem: Cost-Effectiveness and Efficiency**   
Due to the time-intensive nature of the analysis and associated costs, it is crucial to identify and address issues early in the process.   
**Solutions**
  - Fast Fail Approach: Implement mechanisms to quickly determine if there are problems with the test set of questions.
    - This could include identifying if the test set contains an excessive number of questions.
  - Two-Tiered Evaluation Process:
    - Fast Test Set: Begin with a quick evaluation using a top 20 question set that every customer should be able to handle.
      - If the set meets a threshold, such as 90% accuracy on specified metrics, proceed to a more comprehensive analysis.
    - Extended Evaluation:
      - Reserve longer, more detailed tests for periods following major releases rather than after every commit or pull request. This approach ensures that the end product is polished for customer use.

- **Problem: User Interaction Analysis**   
**Solutions**
  - Analytics Integration: Implement analytics tools, like Langfuse, to monitor and understand how users interact with your RAG.

- **Problem: Focus and Relevance of Questions**    
**Solutions**
  - Question Clustering: Organize questions into cluster groups and prioritize evaluation on the top 5 clusters.
  - Topic Modeling: Employ techniques like BERTtopic to effectively group questions into relevant topics for targeted evaluation.

### Literature 
- [Hands-On Large Language Models(Jay Alammar, Maarten Grootendorst)](https://learning.oreilly.com/api/v1/continue/9781098150952/)
