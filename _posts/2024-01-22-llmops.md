---
title: How to speed up a python program?
layout: post
use_toc: true
excerpt: 3 methods to speed up a python program
---
## 1. Before starting any optimizations identify the most problematic parts of Code 

### Python debugging performance
- [scikit-learn recommendations](https://scikit-learn.org/stable/developers/performance.html)
- [Blog which collects useful tools](https://pythonspeed.com/articles/beyond-cprofile/)
- [PyInstrument](https://github.com/joerick/pyinstrument/)      

  ```python
  from pyinstrument import Profiler

  profiler = Profiler()
  profiler.start()

  tm = TMI()
  tm.fit(tree_100_0, tree_100_1)
  profiler.stop()

  print(profiler.output_text(unicode=True, color=True))
  ```    

- [cProfiler](https://ipython-books.github.io/42-profiling-your-code-easily-with-cprofile-and-ipython/)     

  ```
  %%prun -s tottime -q -l 20 -T prun0

  print(open('prun0', 'r').read())
  ```    
  
The %prun and %%prun magic commands accept multiple optional options (type %prun? for more details). 
In the example, 
-s allows us to sort the report by a particular column, -q to suppress (quell) the pager output (which is useful when we want to integrate the output in a notebook), 
-l to limit the number of lines displayed or to filter the results by function name (which is useful when we are interested in a particular function), 
and -T to save the report in a text file. In addition, we can choose to save (dump) 
the binary report in a file with -D, or to return it in IPython with -r. 
This database-like object contains all information about the profiling and can be analyzed through Python's pstats module.

## 2. Cython
- [Online Course from oreilly](https://learning.oreilly.com/videos/learning-cython/)
- [Summary of hot to use Cython](https://byumcl.bitbucket.io/bootcamp2014/_downloads/AllWeek7.pdf)
- Sample code of usage from [scikit-learn ](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx)
- Before starting make sure that `sudo apt-get install g++`
- Use magic method in jupyter nobooks for fast prototyping. Flag -a means to give analysis which shows parts which may be optimized `%%cython -a`
- In order to compile the file and later on import it in your code use: `python setup.py build_ext --inplace`
- [OpenMP Cython Jupyter notebook example](https://homes.cs.washington.edu/~jmschr/lectures/Parallel_Processing_in_Python.html)


## 3. Multithreading in Python
- [Use shared recources instead of copying](https://research.wmz.ninja/articles/2018/03/on-sharing-large-arrays-when-using-pythons-multiprocessing.html)
- There are can be several `Threads` inside one `Process`. `Threads` share resources between each other, while `Process` can be executed in `parallel`: without any sharing.    
- In the multiprocessing module, the `Pool` class is mainly used to implement a pool of processes, each of which will carry out tasks submitted to a Pool object. Generally, the Pool class is more convenient than the `Process` class, especially if the results returned from your concurrent application should be ordered.
`Pool` class: the `Pool.map()` and `Pool.apply()` methods follow the convention of Python's traditional `map()` and `apply()` methods, ensuring that the returned values are ordered in the same way that the input is. These methods, however, block the main program until a process has finished processing. The Pool class, therefore, also has the `map_async()` and `apply_async()` functions to better assist concurrency and parallelism.
- Common issues [1](https://stackoverflow.com/questions/13068760/parallelise-python-loop-with-numpy-arrays-and-shared-memory),[2](https://stackoverflow.com/questions/11368486/openmp-and-python)


[//]: # (## System Design)

[//]: # ()
[//]: # (<div style="display: flex; justify-content: center; padding-top: 20px; padding-bottom: 20px;">)

[//]: # (    <img src="{{ site.baseurl }}/images/LLMOps/system_design.png">)

[//]: # (</div>)

[//]: # ()
[//]: # (Evaluating RAG models involves a systematic process that can be broken down into several steps. Below is a detailed guide based on the key components you've outlined.)

[//]: # ()
[//]: # (### 1. Generate a Dataset of Questions)

[//]: # (#### 1.1 Data Processing)

[//]: # (- **Data**: Select a dataset that is relevant to the domain in which the RAG model will be applied.)

[//]: # (- **Split**: Divide the dataset into manageable chunks to improve the efficiency of the question-generation process.)

[//]: # (- **Chunks**: Individually process each chunk to generate questions.)

[//]: # ()
[//]: # (#### 1.2 Question Generation)

[//]: # (- **Generate 5 questions**: Aim to generate a set of five questions from each data chunk.)

[//]: # (- **Questions for each chunk**: Collate the questions generated from each data chunk.)

[//]: # (- **Create Encoded Embeddings**: Convert the generated questions into vector space embeddings for similarity searches.)

[//]: # (- **Run Similarity Search**: Use the encoded embeddings to identify similar questions or to perform deduplication.)

[//]: # (- **Questions-to-Questions with Score**: Evaluate the similarity search results by scoring the quality and relevance of the questions.)

[//]: # ()
[//]: # (#### 1.3 Question Filtering and Finalization)

[//]: # (- **Filter with Threshold**: Implement a threshold to filter out questions that do not meet the required similarity score, to ensure question uniqueness and relevance.)

[//]: # (- **Cleaned Questions**: Compile the questions that exceed the threshold into a cleaned dataset.)

[//]: # (- **Combine chunks at random and generate questions**: Integrate questions from various chunks randomly to enhance diversity and create a final set of questions.)

[//]: # (- **Add questions generated by multiple chunks**: Incorporate questions that are common across multiple chunks for robustness.)

[//]: # (- **Final Set of Questions &#40;20-100&#41;**: The final output is a refined set of 20 to 100 questions that are ready for the evaluation phase.)

[//]: # ()
[//]: # (### 2. RAG)

[//]: # ()
[//]: # (- **Question as a Vector**: Transform the question into a vector format.)

[//]: # (- **Find the Most Relevant Chunks**: Search the vector store, which is a database of encoded vectors, to retrieve the most pertinent information chunks related to the question.)

[//]: # (- **Context**: Supply the RAG model with context drawn from the retrieved chunks.)

[//]: # (- **Top N Results**: Have the model retrieve the top N most relevant answers or results.)

[//]: # (- **GPT Prompt**: Pass the generated prompt to the GPT component of the RAG model.)

[//]: # (- **Answer**: Utilize GPT to generate an answer based on the given prompt and the context provided.)

[//]: # ()
[//]: # (### 3. Evaluate the Outcome)

[//]: # ()
[//]: # (- **Question**: Reference the original question for comparison.)

[//]: # (- **Answer**: Examine the answer generated by the RAG model.)

[//]: # (- **Context**: Verify the accuracy and relevancy of the context used to produce the answer.)

[//]: # (- **Prompt**: Confirm that the prompt was appropriately formed and is pertinent to the question.)

[//]: # (- **RAGAs, BertScore, etc.**: Employ evaluation metrics such as RAGAs and BertScore to assess the quality of the answers.)

[//]: # (- **Scores**: Aggregate the scores from the different metrics to gauge the RAG model's performance.)

[//]: # ()
[//]: # (The aforementioned steps are essential to ensure that the RAG model is thoroughly and accurately tested.)

[//]: # ()
[//]: # (## Dashboard)

[//]: # (<div style="display: flex; justify-content: center; padding-top: 20px; padding-bottom: 20px;">)

[//]: # (    <img src="{{ site.baseurl }}/images/LLMOps/dashboard.png">)

[//]: # (</div>)

[//]: # (Dashboard could be implemented using superset or metabase.)

[//]: # ()
[//]: # (## Parameters)

[//]: # (### Chunking)

[//]: # (  - Chunk size)

[//]: # (  - Chunk overlap)

[//]: # (  - Embedding model)

[//]: # (### Retrival)

[//]: # (  - Number of results)

[//]: # (  - Similarity threshold)

[//]: # (  - Retrival Strategy &#40;BM25, cosine similarity, hybrid etc.&#41;)

[//]: # (  - Reranking)

[//]: # (### Generation)

[//]: # (  - LLM Model)

[//]: # (  - Prompt)

[//]: # (  - Temperature)

[//]: # ()
[//]: # (## Metrics)

[//]: # (### Chunking)

[//]: # (this is difficult to measure, therefore we can only evaluate end-to-end performance. )

[//]: # ()
[//]: # (### Retrival)

[//]: # (focuses on how accurately we can retrieve necessary chunks from vector store.)

[//]: # (- #### Mean Average Precision)

[//]: # (- evaluates System 1’s result for the query to be 0.6 and System 2’s to be 0.1. )

[//]: # (![image]&#40;./images/LLMOps/retrival.png&#41;)

[//]: # (- #### Ragas)

[//]: # (  - Context Precision)

[//]: # (    - evaluates whether all of the ground-truth relevant items present in the `context` are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the `question` and the `contexts`.)

[//]: # ()
[//]: # (### Generation)

[//]: # (- #### BLEU/ROUGE)

[//]: # (- #### BERTScore)

[//]: # (- #### Ragas)

[//]: # (  - Answer Relevance )

[//]: # (    - focuses on assessing how pertinent the generated answer is to the given prompt. This metric is computed using the `question` and the `answer`.     )

[//]: # (  - Faithfulness)

[//]: # (    - This measures the factual consistency of the generated answer against the given context. It is calculated from `answer` and retrieved `context`.)

[//]: # ()
[//]: # (## Problems:)

[//]: # (### Problem: Evaluation Speed)

[//]: # (- The current evaluation process is slow, taking approximately one minute per question for all metrics.)

[//]: # (### Solutions)

[//]: # (- **Parallel Processing**: Run evaluations in parallel for each question to generate tokens more efficiently.)

[//]: # (- **Threading**: Utilize threads in newer models to enhance processing speed.)

[//]: # ()
[//]: # (### Problem: Cost-Effectiveness and Efficiency)

[//]: # (- Due to the time-intensive nature of the analysis and associated costs, it is crucial to identify and address issues early in the process.)

[//]: # (### Solutions)

[//]: # (- **Fast Fail Approach**: Implement mechanisms to quickly determine if there are problems with the test set of questions.)

[//]: # (    - This could include identifying if the test set:)

[//]: # (        - Contains an excessive number of questions.)

[//]: # (- **Two-Tiered Evaluation Process**:)

[//]: # (    - **Fast Test Set**: Begin with a quick evaluation using a top 20 question set that every customer should be able to handle.)

[//]: # (        - If the set meets a threshold, such as 90% accuracy on specified metrics, proceed to a more comprehensive analysis.)

[//]: # (    - **Extended Evaluation**:)

[//]: # (        - Reserve longer, more detailed tests for periods following major releases rather than after every commit or pull request. This approach ensures that the end product is polished for customer use.)

[//]: # ()
[//]: # (### Problem: User Interaction Analysis)

[//]: # (### Solutions)

[//]: # (- **Analytics Integration**: Implement analytics tools, like Langfuse, to monitor and understand how users interact with your RAG.)

[//]: # ()
[//]: # (## Problem: Focus and Relevance of Questions)

[//]: # (### Solutions)

[//]: # (- **Question Clustering**: Organize questions into cluster groups and prioritize evaluation on the top 5 clusters.)

[//]: # (- **Topic Modeling**: Employ techniques like Berttopic to effectively group questions into relevant topics for targeted evaluation.)

[//]: # ()
[//]: # (### Literature )

[//]: # (- [Hands-On Large Language Models&#40;Jay Alammar, Maarten Grootendorst&#41;]&#40;https://learning.oreilly.com/api/v1/continue/9781098150952/&#41;)
